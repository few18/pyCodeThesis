{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b64f0305",
   "metadata": {},
   "source": [
    "## Functions\n",
    "In this part all the funcitons and methods used to analyse the data is defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41784f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the required libraries\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import functools\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import scipy as sc\n",
    "import numpy.matlib as npm\n",
    "import sys\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "697306df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Easiest way to make code work was to copy source code into file, easier than figuring out paths\n",
    "especially on the maths cluster.\n",
    "\"\"\"\n",
    "\n",
    "class DeepESN():\n",
    "\n",
    "    '''\n",
    "    Deep Echo State Network (DeepESN) class:\n",
    "    this class implement the DeepESN model suitable for\n",
    "    time-serie prediction and sequence classification.\n",
    "    Reference paper for DeepESN model:\n",
    "    C. Gallicchio, A. Micheli, L. Pedrelli, \"Deep Reservoir Computing: A\n",
    "    Critical Experimental Analysis\", Neurocomputing, 2017, vol. 268, pp. 87-99\n",
    "\n",
    "    Reference paper for the design of DeepESN model in multivariate time-series prediction tasks:\n",
    "    C. Gallicchio, A. Micheli, L. Pedrelli, \"Design of deep echo state networks\",\n",
    "    Neural Networks, 2018, vol. 108, pp. 33-47\n",
    "\n",
    "    ----\n",
    "    This file is a part of the DeepESN Python Library (DeepESNpy)\n",
    "    Luca Pedrelli\n",
    "    luca.pedrelli@di.unipi.it\n",
    "    lucapedrelli@gmail.com\n",
    "    Department of Computer Science - University of Pisa (Italy)\n",
    "    Computational Intelligence & Machine Learning (CIML) Group\n",
    "    http://www.di.unipi.it/groups/ciml/\n",
    "    ----\n",
    "    '''\n",
    "\n",
    "    def __init__(self, Nu,Nr,Nl, configs, verbose=0):\n",
    "        # initialize the DeepESN model\n",
    "\n",
    "        if verbose:\n",
    "            sys.stdout.write('init DeepESN...')\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        rhos = np.array(configs.rhos) # spectral radius (maximum absolute eigenvalue)\n",
    "        lis = np.array(configs.lis) # leaky rate\n",
    "        iss = np.array(configs.iss) # input scale\n",
    "        IPconf = configs.IPconf # configuration for Deep Intrinsic Plasticity\n",
    "        reservoirConf = configs.reservoirConf # reservoir configurations\n",
    "\n",
    "        if len(rhos.shape) == 0:\n",
    "            rhos = npm.repmat(rhos, 1,Nl)[0]\n",
    "\n",
    "        if len(lis.shape) == 0:\n",
    "            lis = npm.repmat(lis, 1,Nl)[0]\n",
    "\n",
    "        if len(iss.shape) == 0:\n",
    "            iss = npm.repmat(iss, 1,Nl)[0]\n",
    "\n",
    "        self.W = {} # recurrent weights\n",
    "        self.Win = {} # recurrent weights\n",
    "        self.Gain = {} # activation function gain\n",
    "        self.Bias = {} # activation function bias\n",
    "\n",
    "        self.Nu = Nu # number of inputs\n",
    "        self.Nr = Nr # number of units per layer\n",
    "        self.Nl = Nl # number of layers\n",
    "        self.rhos = rhos.tolist() # list of spectral radius\n",
    "        self.lis = lis # list of leaky rate\n",
    "        self.iss = iss # list of input scale\n",
    "\n",
    "        self.IPconf = IPconf\n",
    "\n",
    "        self.readout = configs.readout\n",
    "\n",
    "        # sparse recurrent weights init\n",
    "        if reservoirConf.connectivity < 1:\n",
    "            for layer in range(Nl):\n",
    "                self.W[layer] = np.zeros((Nr,Nr))\n",
    "                for row in range(Nr):\n",
    "                    number_row_elements = round(reservoirConf.connectivity * Nr)\n",
    "                    row_elements = random.sample(range(Nr), number_row_elements)\n",
    "                    self.W[layer][row,row_elements] = np.random.uniform(-1,+1, size = (1,number_row_elements))\n",
    "\n",
    "        # full-connected recurrent weights init\n",
    "        else:\n",
    "            for layer in range(Nl):\n",
    "                self.W[layer] = np.random.uniform(-1,+1, size = (Nr,Nr))\n",
    "\n",
    "        # layers init\n",
    "        for layer in range(Nl):\n",
    "\n",
    "            target_li = lis[layer]\n",
    "            target_rho = rhos[layer]\n",
    "            input_scale = iss[layer]\n",
    "\n",
    "            if layer==0:\n",
    "                self.Win[layer] = np.random.uniform(-input_scale, input_scale, size=(Nr,Nu+1))\n",
    "            else:\n",
    "                self.Win[layer] = np.random.uniform(-input_scale, input_scale, size=(Nr,Nr+1))\n",
    "\n",
    "            Ws = (1-target_li) * np.eye(self.W[layer].shape[0], self.W[layer].shape[1]) + target_li * self.W[layer]\n",
    "            eig_value,eig_vector = np.linalg.eig(Ws)\n",
    "            actual_rho = np.max(np.absolute(eig_value))\n",
    "\n",
    "            Ws = (Ws *target_rho)/actual_rho\n",
    "            self.W[layer] = (target_li**-1) * (Ws - (1.-target_li) * np.eye(self.W[layer].shape[0], self.W[layer].shape[1]))\n",
    "\n",
    "            self.Gain[layer] = np.ones((Nr,1))\n",
    "            self.Bias[layer] = np.zeros((Nr,1))\n",
    "\n",
    "        if verbose:\n",
    "            print('done.')\n",
    "            sys.stdout.flush()\n",
    "\n",
    "    def computeLayerState(self, input, layer, initialStatesLayer = None, DeepIP = 0):\n",
    "        # compute the state of a layer with pre-training if DeepIP == 1\n",
    "\n",
    "        state = np.zeros((self.Nr, input.shape[1]))\n",
    "\n",
    "        if initialStatesLayer is None:\n",
    "            initialStatesLayer = np.zeros(state[:,0:1].shape)\n",
    "\n",
    "        input = self.Win[layer][:,0:-1].dot(input) + np.expand_dims(self.Win[layer][:,-1],1)\n",
    "\n",
    "        if DeepIP:\n",
    "            state_net = np.zeros((self.Nr, input.shape[1]))\n",
    "            state_net[:,0:1] = input[:,0:1]\n",
    "            state[:,0:1] = self.lis[layer] * np.tanh(np.multiply(self.Gain[layer], state_net[:,0:1]) + self.Bias[layer])\n",
    "        else:\n",
    "            #state[:,0:1] = self.lis[layer] * np.tanh(np.multiply(self.Gain[layer], input[:,0:1]) + self.Bias[layer])\n",
    "            state[:,0:1] = (1-self.lis[layer]) * initialStatesLayer + self.lis[layer] * np.tanh( np.multiply(self.Gain[layer], self.W[layer].dot(initialStatesLayer) + input[:,0:1]) + self.Bias[layer])\n",
    "\n",
    "        for t in range(1,state.shape[1]):\n",
    "            if DeepIP:\n",
    "                state_net[:,t:t+1] = self.W[layer].dot(state[:,t-1:t]) + input[:,t:t+1]\n",
    "                state[:,t:t+1] = (1-self.lis[layer]) * state[:,t-1:t] + self.lis[layer] * np.tanh(np.multiply(self.Gain[layer], state_net[:,t:t+1]) + self.Bias[layer])\n",
    "\n",
    "                eta = self.IPconf.eta\n",
    "                mu = self.IPconf.mu\n",
    "                sigma2 = self.IPconf.sigma**2\n",
    "\n",
    "                # IP learning rule\n",
    "                deltaBias = -eta*((-mu/sigma2)+ np.multiply(state[:,t:t+1], (2*sigma2+1-(state[:,t:t+1]**2)+mu*state[:,t:t+1])/sigma2))\n",
    "                deltaGain = eta / npm.repmat(self.Gain[layer],1,state_net[:,t:t+1].shape[1]) + deltaBias * state_net[:,t:t+1]\n",
    "\n",
    "                # update gain and bias of activation function\n",
    "                self.Gain[layer] = self.Gain[layer] + deltaGain\n",
    "                self.Bias[layer] = self.Bias[layer] + deltaBias\n",
    "\n",
    "            else:\n",
    "                state[:,t:t+1] = (1-self.lis[layer]) * state[:,t-1:t] + self.lis[layer] * np.tanh( np.multiply(self.Gain[layer], self.W[layer].dot(state[:,t-1:t]) + input[:,t:t+1]) + self.Bias[layer])\n",
    "\n",
    "        return state\n",
    "\n",
    "    def computeDeepIntrinsicPlasticity(self, inputs):\n",
    "        # we incrementally perform the pre-training (deep intrinsic plasticity) over layers\n",
    "\n",
    "        len_inputs = range(len(inputs))\n",
    "        states = []\n",
    "\n",
    "        for i in len_inputs:\n",
    "            states.append(np.zeros((self.Nr*self.Nl, inputs[i].shape[1])))\n",
    "\n",
    "        for layer in range(self.Nl):\n",
    "\n",
    "            for epoch in range(self.IPconf.Nepochs):\n",
    "                Gain_epoch = self.Gain[layer]\n",
    "                Bias_epoch = self.Bias[layer]\n",
    "\n",
    "\n",
    "                if len(inputs) == 1:\n",
    "                    self.computeLayerState(inputs[0][:,self.IPconf.indexes], layer, DeepIP = 1)\n",
    "                else:\n",
    "                    for i in self.IPconf.indexes:\n",
    "                        self.computeLayerState(inputs[i], layer, DeepIP = 1)\n",
    "\n",
    "\n",
    "                if (np.linalg.norm(self.Gain[layer]-Gain_epoch,2) < self.IPconf.threshold) and (np.linalg.norm(self.Bias[layer]-Bias_epoch,2)< self.IPconf.threshold):\n",
    "                    sys.stdout.write(str(epoch+1))\n",
    "                    sys.stdout.write('.')\n",
    "                    sys.stdout.flush()\n",
    "                    break\n",
    "\n",
    "                if epoch+1 == self.IPconf.Nepochs:\n",
    "                    sys.stdout.write(str(epoch+1))\n",
    "                    sys.stdout.write('.')\n",
    "                    sys.stdout.flush()\n",
    "\n",
    "            inputs2 = []\n",
    "            for i in range(len(inputs)):\n",
    "                inputs2.append(self.computeLayerState(inputs[i], layer))\n",
    "\n",
    "            for i in range(len(inputs)):\n",
    "                states[i][(layer)*self.Nr: (layer+1)*self.Nr,:] = inputs2[i]\n",
    "\n",
    "            inputs = inputs2\n",
    "\n",
    "        return states\n",
    "\n",
    "    def computeState(self,inputs, DeepIP = 0, initialStates = None, verbose=0):\n",
    "        # compute the global state of DeepESN with pre-training if DeepIP == 1\n",
    "\n",
    "        if self.IPconf.DeepIP and DeepIP:\n",
    "            if verbose:\n",
    "                sys.stdout.write('compute state with DeepIP...')\n",
    "                sys.stdout.flush()\n",
    "            states = self.computeDeepIntrinsicPlasticity(inputs)\n",
    "        else:\n",
    "            if verbose:\n",
    "                sys.stdout.write('compute state...')\n",
    "                sys.stdout.flush()\n",
    "            states = []\n",
    "\n",
    "            for i_seq in range(len(inputs)):\n",
    "                states.append(self.computeGlobalState(inputs[i_seq], initialStates))\n",
    "\n",
    "        if verbose:\n",
    "            print('done.')\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        return states\n",
    "\n",
    "    def computeGlobalState(self,input, initialStates):\n",
    "        # compute the global state of DeepESN\n",
    "\n",
    "        state = np.zeros((self.Nl*self.Nr,input.shape[1]))\n",
    "\n",
    "        initialStatesLayer = None\n",
    "\n",
    "\n",
    "        for layer in range(self.Nl):\n",
    "            if initialStates is not None:\n",
    "                initialStatesLayer = initialStates[(layer)*self.Nr: (layer+1)*self.Nr,:]\n",
    "            state[(layer)*self.Nr: (layer+1)*self.Nr,:] = self.computeLayerState(input, layer, initialStatesLayer, 0)\n",
    "            input = state[(layer)*self.Nr: (layer+1)*self.Nr,:]\n",
    "\n",
    "        return state\n",
    "\n",
    "    def trainReadout(self,trainStates,trainTargets,lb, verbose=0):\n",
    "        # train the readout of DeepESN\n",
    "\n",
    "        trainStates = np.concatenate(trainStates,1)\n",
    "        trainTargets = np.concatenate(trainTargets,1)\n",
    "\n",
    "        # add bias\n",
    "        X = np.ones((trainStates.shape[0]+1, trainStates.shape[1]))\n",
    "        X[:-1,:] = trainStates\n",
    "        trainStates = X\n",
    "\n",
    "        if verbose:\n",
    "            sys.stdout.write('train readout...')\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        if self.readout.trainMethod == 'SVD': # SVD, accurate method\n",
    "            U, s, V = np.linalg.svd(trainStates, full_matrices=False);\n",
    "            s = s/(s**2 + lb)\n",
    "\n",
    "            self.Wout = trainTargets.dot(np.multiply(V.T, np.expand_dims(s,0)).dot(U.T));\n",
    "\n",
    "        else:  # NormalEquation, fast method\n",
    "            B = trainTargets.dot(trainStates.T)\n",
    "            A = trainStates.dot(trainStates.T)\n",
    "\n",
    "            self.Wout = np.linalg.solve((A + np.eye(A.shape[0], A.shape[1]) * lb), B.T).T\n",
    "\n",
    "        if verbose:\n",
    "            print('done.')\n",
    "            sys.stdout.flush()\n",
    "\n",
    "    def computeOutput(self,state):\n",
    "        # compute a linear combination between the global state and the output weights\n",
    "        state = np.concatenate(state,1)\n",
    "        return self.Wout[:,0:-1].dot(state) + np.expand_dims(self.Wout[:,-1],1) # Wout product + add bias\n",
    "\n",
    "# Define deep esn function for getting correlation\n",
    "# Modification of Luca Pedrelli's code\n",
    "def Causal_ESN_deep(X,Y):\n",
    "\n",
    "    class Struct(object): pass\n",
    "\n",
    "    def config_data(IP_indexes):\n",
    "\n",
    "        configs = Struct()\n",
    "\n",
    "        configs.rhos = 0.1 # set spectral radius 0.1 for all recurrent layers\n",
    "        configs.lis = [0.1,0.5,0.9] # set li to 0.1,0.5,0.9 for the recurrent layers\n",
    "        configs.iss = 1 # set insput scale 0.1 for all recurrent layers\n",
    "\n",
    "        configs.IPconf = Struct()\n",
    "        configs.IPconf.DeepIP = 0 # activate pre-train\n",
    "        configs.IPconf.threshold = 0.1 # threshold for gradient descent in pre-train algorithm\n",
    "        configs.IPconf.eta = 10e-5 # learning rate for IP rule\n",
    "        configs.IPconf.mu = 0 # mean of target gaussian function\n",
    "        configs.IPconf.sigma = 0.1 # std of target gaussian function\n",
    "        configs.IPconf.Nepochs = 10 # maximum number of epochs\n",
    "        configs.IPconf.indexes = IP_indexes # perform the pre-train on these indexes\n",
    "\n",
    "        configs.reservoirConf = Struct()\n",
    "        configs.reservoirConf.connectivity = 1 # connectivity of recurrent matrix\n",
    "\n",
    "        configs.readout = Struct()\n",
    "        configs.readout.trainMethod = 'SVD'\n",
    "        configs.readout.regularizations = 10.0**np.array(range(-4,-1,1))\n",
    "\n",
    "        return configs\n",
    "\n",
    "    def select_indexes(data, indexes, transient=0):\n",
    "\n",
    "        if len(data) == 1:\n",
    "            return [data[0][:,indexes][:,transient:]]\n",
    "\n",
    "        return [data[i][:,transient:] for i in indexes]\n",
    "\n",
    "    def load_data(X, Y):\n",
    "\n",
    "        X = np.reshape(X,(1,X.shape[0]))\n",
    "        Y = np.reshape(Y,(1,Y.shape[0]))\n",
    " \n",
    "        dataset = Struct()\n",
    "        dataset.name = ['DATA']\n",
    "        dataset.inputs = [X]\n",
    "        dataset.targets = [Y]\n",
    "\n",
    "        # input dimension\n",
    "        Nu = dataset.inputs[0].shape[0]\n",
    "\n",
    "\n",
    "        # select the model that achieves the maximum accuracy on validation set\n",
    "        optimization_problem = np.argmin\n",
    "\n",
    "        # set size of training set\n",
    "        tr_ind = int(X.shape[1]*0.8)\n",
    "\n",
    "        TR_indexes = range(tr_ind) \n",
    "        VL_indexes = range(tr_ind,X.shape[1])\n",
    "        TS_indexes = range(Y.shape[1])\n",
    "\n",
    "        return dataset, Nu, optimization_problem, TR_indexes, VL_indexes, TS_indexes\n",
    "\n",
    "    dataset, Nu, optimization_problem, TR_indexes, VL_indexes, TS_indexes = load_data(X, Y)\n",
    "\n",
    "    # load configuration for pianomidi task\n",
    "    configs = config_data(list(TR_indexes) + list(VL_indexes))\n",
    "\n",
    "    Nr = 100 # number of recurrent units\n",
    "    Nl = 3 # number of recurrent layers\n",
    "    reg = 10.0e-6\n",
    "    transient = 5\n",
    "\n",
    "    deepESN = DeepESN(Nu, Nr, Nl, configs, verbose = 0)\n",
    "    states = deepESN.computeState(dataset.inputs, deepESN.IPconf.DeepIP)\n",
    "\n",
    "    train_states = select_indexes(states, list(TR_indexes) + list(VL_indexes), transient)\n",
    "    train_targets = select_indexes(dataset.targets, list(TR_indexes) + list(VL_indexes), transient)\n",
    "    test_states = select_indexes(states, TS_indexes)\n",
    "    test_targets = select_indexes(dataset.targets, TS_indexes)\n",
    "    deepESN.trainReadout(train_states, train_targets, reg)\n",
    "\n",
    "    train_outputs = deepESN.computeOutput(train_states)\n",
    "\n",
    "    test_outputs = deepESN.computeOutput(test_states)\n",
    "\n",
    "    corr = scipy.stats.spearmanr(test_outputs, test_targets[0], axis = 1)\n",
    "\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2995308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to run surrogate time series through deep esn\n",
    "def compute_confidence(X_data, Y_data, verbose = False):\n",
    "\n",
    "    n, T = X_data.shape\n",
    "\n",
    "    lags = np.arange(0,21)\n",
    "    xmapy = np.zeros((n,21))\n",
    "    ymapx = np.zeros((n,21))\n",
    "\n",
    "    for j in range(n):\n",
    "        if verbose:\n",
    "            if (j%20 == 0):\n",
    "                print(\"Current Iteration {}\".format(j))\n",
    "        for i in range(21):\n",
    "\n",
    "            if i < 11:\n",
    "                L1 = 10 - i\n",
    "                R1 = T\n",
    "                L2 = 0\n",
    "                R2 = T - 10 + i\n",
    "\n",
    "            else:\n",
    "                L1 = 0\n",
    "                R1 = T - i + 10\n",
    "                L2 = i - 10\n",
    "                R2 = T\n",
    "\n",
    "            X = X_data[j,L1:R1]\n",
    "            Y = Y_data[j,L2:R2]\n",
    "            xmapy[j,i] = Causal_ESN_deep(X,Y)[0]\n",
    "\n",
    "            Y = Y_data[j,L1:R1]\n",
    "            X = X_data[j,L2:R2]\n",
    "            ymapx[j,i] = Causal_ESN_deep(Y,X)[0]\n",
    "\n",
    "    return xmapy, ymapx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f5ae13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to run actual time series through deep esn\n",
    "def compute_lags(X_data, Y_data, n, verbose = False):\n",
    "\n",
    "    T = X_data.shape[0]\n",
    "\n",
    "    lags = np.arange(0,21)\n",
    "    xmapy = np.zeros((n,21))\n",
    "    ymapx = np.zeros((n,21))\n",
    "\n",
    "    for j in range(n):\n",
    "        if verbose:\n",
    "            if (j%20 == 0):\n",
    "                print(\"Current Iteration {}\".format(j))\n",
    "        for i in range(21):\n",
    "\n",
    "            if i < 11:\n",
    "                L1 = 10 - i\n",
    "                R1 = T\n",
    "                L2 = 0\n",
    "                R2 = T - 10 + i\n",
    "\n",
    "            else:\n",
    "                L1 = 0\n",
    "                R1 = T - i + 10\n",
    "                L2 = i - 10\n",
    "                R2 = T\n",
    "\n",
    "            X = X_data[L1:R1]\n",
    "            Y = Y_data[L2:R2]\n",
    "            xmapy[j,i] = Causal_ESN_deep(X,Y)[0]\n",
    "\n",
    "            Y = Y_data[L1:R1]\n",
    "            X = X_data[L2:R2]\n",
    "            ymapx[j,i] = Causal_ESN_deep(Y,X)[0]\n",
    "\n",
    "    return xmapy, ymapx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "801a4ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ci(data):\n",
    "\n",
    "    n = data.shape[1]\n",
    "    se = scipy.stats.sem(data, axis = 0)\n",
    "    h = se * scipy.stats.t.ppf(1.95 / 2., n-1)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0686f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lag_fig(xmapy_surr, ymapx_surr, xmapy, ymapx, x_name, y_name):\n",
    "    x = np.arange(-10,11)\n",
    "\n",
    "    mean_xmapy = xmapy.mean(axis = 0)\n",
    "    mean_ymapx = ymapx.mean(axis = 0)\n",
    "\n",
    "    ci_xmapy = compute_ci(xmapy)\n",
    "    ci_ymapx = compute_ci(ymapx)\n",
    "\n",
    "    fig = plt.figure(figsize = (8,6))\n",
    "    \n",
    "#     name_x = x_name[:3]\n",
    "#     name_y = y_name[:3]\n",
    "\n",
    "    plt.plot(x, mean_xmapy, 'C0-', label= r'Y $\\rightarrow$ X')\n",
    "    plt.plot(x, xmapy_surr.mean(axis = 0), 'C0--', label = r'Y $\\rightarrow$ X surrogate')\n",
    "    plt.fill_between(x, mean_xmapy - ci_xmapy, mean_xmapy + ci_xmapy, color='C0', alpha=0.2)\n",
    "\n",
    "    plt.plot(x, mean_ymapx, 'C1-', label= r'X $\\rightarrow$ Y')\n",
    "    plt.plot(x, ymapx_surr.mean(axis = 0), 'C1--', label = r'X $\\rightarrow$ Y surrogate')\n",
    "    plt.fill_between(x, mean_ymapx - ci_ymapx, mean_ymapx + ci_ymapx, color='C1', alpha=0.2)\n",
    "\n",
    "    plt.xticks(x)\n",
    "    plt.grid(linestyle = '--', alpha = 0.5)\n",
    "\n",
    "    plt.ylabel(r'$\\rho$', rotation = 0)\n",
    "    plt.xlabel(r'$\\tau$')\n",
    "    #plt.title('Correlation vs Lag')\n",
    "    plt.legend(loc = 1)\n",
    "    #plt.savefig('test.png', dpi=300)\n",
    "\n",
    "#     return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1ce493",
   "metadata": {},
   "source": [
    "## Data\n",
    "In this next part the data is generated for the analysis. <br>\n",
    "In this notebook a weakly coupled uni-directional logistic map is investigated, defined by:\n",
    "\n",
    "$\n",
    "x(t+1) = x(t)[3.78 - 3.78x(t)] \\\\\n",
    "y(t+1) = y(t)[3.1 - 3.1y(t) - 0.8x(t - \\tau)]\n",
    "$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00fa93f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell that simulates the above described uni-directionally forced logistic map.\n",
    "\"\"\"\n",
    "# initialise x and y arrays\n",
    "x = np.zeros(10000)\n",
    "y = np.zeros(10000)\n",
    "# set inital conditions\n",
    "x[0] = 0.2\n",
    "y[0] = 0.4\n",
    "# run through first couple of instances to get process going\n",
    "for i in range(1,13):\n",
    "    x[i] = x[i-1] * (3.78 - 3.78*x[i-1])\n",
    "    y[i] = y[i-1] * (3.1 - 3.1*y[i-1])\n",
    "# run for rest of length with the coupling\n",
    "for i in range(13, 10000):\n",
    "    x[i] = x[i-1] * (3.78 - 3.78*x[i-1])\n",
    "    y[i] = y[i-1] * (3.77 - 3.77*y[i-1] - 0.8*x[i-5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea10605c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get artificial time series of length 250, 500 and 1000 by slicing from above\n",
    "generated data.\n",
    "\"\"\"\n",
    "x_250 = x[9750:]\n",
    "y_250 = y[9750:]\n",
    "\n",
    "x_500 = x[9500:]\n",
    "y_500 = y[9500:]\n",
    "\n",
    "x_1000 = x[9000:]\n",
    "y_1000 = y[9000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4e0cbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardise time series\n",
    "from sklearn import preprocessing\n",
    "\n",
    "x_250 = preprocessing.scale(x_250)\n",
    "y_250 = preprocessing.scale(y_250)\n",
    "\n",
    "x_500 = preprocessing.scale(x_500)\n",
    "y_500 = preprocessing.scale(y_500)\n",
    "\n",
    "x_1000 = preprocessing.scale(x_1000)\n",
    "y_1000 = preprocessing.scale(y_1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcbb400",
   "metadata": {},
   "source": [
    "## Run ESN Method\n",
    "Run the ESN method on the generated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0797fdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "import pyEDM\n",
    "import seaborn as sns\n",
    "from joblib import Parallel, delayed\n",
    "from numpy.random import default_rng\n",
    "from scipy.spatial.distance import squareform, pdist\n",
    "\n",
    "\n",
    "\n",
    "def recurrence_matrix(x_e, quantile_per=12.5):\n",
    "    \"\"\"\n",
    "    Generates binary recurrence matrix, where the closest ``quantile_per``% of pairwise distances are assigned a 1,\n",
    "    and all other pairs assigned a 0.\n",
    "    :param x_e: embedded dataframe of the required form such as that generated through ``pyEDM.Embed()``\n",
    "    :param quantile_per: percentile of distances to set equal to 1\n",
    "    :return: dataframe of recurrence matrix\n",
    "    \"\"\"\n",
    "    dist_mat = squareform(pdist(x_e.values, metric=\"chebyshev\"))\n",
    "    binary_dist_mtx = (dist_mat <= np.percentile(dist_mat, quantile_per)).astype(int)\n",
    "    # print(\"1s: \", np.count_nonzero(binary_dist_mtx) / (len(binary_dist_mtx) ** 2))\n",
    "    return binary_dist_mtx\n",
    "\n",
    "def twins_list(len_df, Ng, r_mtx, obs_per_year=12):\n",
    "    \"\"\"\n",
    "    Creates an array of all twin points. Twin points are defined as those which are sufficiently close together in\n",
    "    state space (same column in the recurrence matrix) and have the same seasonality (measurements from the same month)\n",
    "    :param len_df: length of original dataframe (i.e. length of time series)\n",
    "    :param Ng: length of embedded dataframe\n",
    "    :param r_mtx: recurrence matrix dataframe\n",
    "    :param obs_per_year: seasonality of the data\n",
    "    :return: array of twins, formatted in two different ways. A_arr is the useful format for what follows\n",
    "    \"\"\"\n",
    "    Eminus1 = len_df - Ng\n",
    "    Alist = [[i] for i in range(len_df)]\n",
    "    for i, j in combinations(range(Ng), 2):  # does not include repeats\n",
    "        if np.array_equal(r_mtx[:, i], r_mtx[:, j]) and (j - i) % obs_per_year == 0:\n",
    "            Alist[i + Eminus1].append(j + Eminus1)\n",
    "            Alist[j + Eminus1].append(i + Eminus1)\n",
    "    Q = np.array([len(twins) for twins in Alist])\n",
    "    A_arr = (\n",
    "        np.zeros((len_df, np.max(Q)), dtype=int) - 1000\n",
    "    )  # -1000 here after trying it\n",
    "    for i in range(len_df):\n",
    "        A_arr[i, : Q[i]] = Alist[i]\n",
    "    return A_arr, Q\n",
    "\n",
    "\n",
    "\n",
    "def network_surrogates(len_df, twins_arr, Q, Ng, M=100, obs_per_year=12):\n",
    "    \"\"\"\n",
    "    Generate all surrogates using a network method described in report. First generate points in phase space,\n",
    "    and then append on initial time series\n",
    "    :param len_df: length of original dataframe (i.e. length of time series)\n",
    "    :param twins_arr: array of the twin points of every point (e.g. ouput of twins list\n",
    "    :param Q: array of degrees of each node\n",
    "    :param Ng: length of embedded dataframe/ number of nodes\n",
    "    :param M: number of walkers starting on each node\n",
    "    :param obs_per_year: sampling observations per year of data\n",
    "    :return: arrays for which each column is a surrogate\n",
    "    \"\"\"\n",
    "    nodes = np.arange(Ng - 2)\n",
    "    Eminus1 = len_df - Ng\n",
    "    start_nodes = nodes[nodes % obs_per_year == 0]\n",
    "    start_nodes = start_nodes[start_nodes >= obs_per_year]\n",
    "    num_nodes = len(start_nodes)\n",
    "    X = np.zeros((Ng, M, num_nodes), dtype=int)\n",
    "    X[0, :, :] = np.tile(start_nodes, (M, 1))\n",
    "\n",
    "    rng = default_rng()  # generate random numbers (using numpy generator method)\n",
    "    rand1 = rng.random((Ng, M, num_nodes))\n",
    "\n",
    "    for i in range(1, Ng):\n",
    "        X[i, :, :] = X[i - 1, :, :]\n",
    "        QX = Q[X[i, :, :]]  # degrees of nodes where each walker is currently\n",
    "        Ri = (\n",
    "            rand1[i, :, :] * QX\n",
    "        )  # Generate array of random numbers, each less than QX[i,j]\n",
    "        Ri = Ri.astype(int)  # integer conversion\n",
    "        mask = X[i, :, :] < (Ng - 1)  # relax this condition to let points jump off\n",
    "        X[i, :, :][mask] = twins_arr[X[i, :, :], Ri][mask]  # update walker locations\n",
    "        X[i, :, :][X[i, :, :] < (Ng - 1)] += 1\n",
    "    surrogates = X[:, X[-1, :, :] < (Ng - 1)]\n",
    "    # back propagate initial times to get to full length of time series\n",
    "    initial_times = np.add(\n",
    "        np.tile(surrogates[0, :] - Eminus1, (Eminus1, 1)),\n",
    "        np.mgrid[:Eminus1, : len(surrogates[0, :])][0],\n",
    "    )\n",
    "    return np.concatenate((initial_times, surrogates))  # each column is a surrogate!!\n",
    "\n",
    "\n",
    "def run_surrogates(df, mae_dict, sp, n_surrogates=5, obs_per_year=12, thresholds=-1):\n",
    "    \"\"\"\n",
    "    Generates all the surrogates using the rEDM method. Note the ability to insert a list of\n",
    "    thresholds to try. Default is the rEDM implementation list of thresholds to try.\n",
    "    :param df: input dataframe\n",
    "    :param mae_dict: dictionary of lists; generate with :meth:`src.processing.embedding_dimension.mae_dictionary`\n",
    "    :param sp: species for which to generate surrogates\n",
    "    :param n_surrogates: number of surrogates to generate\n",
    "    :param obs_per_year: period of seasonality of data\n",
    "    :param thresholds: list of thresholds to try. Numbers between 5 and 20 recommended.\n",
    "    :return: embedded dataframe, surrogates, optimal embedding dimension\n",
    "    \"\"\"\n",
    "    surrogate_slice = None\n",
    "    optE = 0\n",
    "    len_df = len(df)\n",
    "    for count in range(25):\n",
    "        optE = mae_dict[sp][count][0]\n",
    "        x_e = pyEDM.Embed(dataFrame=df, E=optE, columns=sp)\n",
    "        Ng = len(x_e)\n",
    "        if thresholds == -1:\n",
    "            thresholds = [\n",
    "                12.5,\n",
    "                12,\n",
    "                11,\n",
    "                10,\n",
    "                9,\n",
    "                8,\n",
    "                7,\n",
    "                6,\n",
    "                5,\n",
    "                15,\n",
    "                16,\n",
    "                17,\n",
    "                18,\n",
    "                19,\n",
    "                20,\n",
    "                4,\n",
    "            ]\n",
    "        twins_arr = None\n",
    "        Q = None\n",
    "        for thres in thresholds:\n",
    "            r_mtx = recurrence_matrix(x_e, thres)\n",
    "            twins_arr, Q = twins_list(\n",
    "                len_df, Ng=Ng, r_mtx=r_mtx, obs_per_year=obs_per_year\n",
    "            )\n",
    "            # if there are at least 10 twins\n",
    "            if np.sum(Q) - Ng > 10:\n",
    "                break\n",
    "        surrogate_slice = network_surrogates(\n",
    "            len_df, twins_arr, Q, Ng, M=100 * n_surrogates, obs_per_year=obs_per_year\n",
    "        )\n",
    "        if len(surrogate_slice[0, :]) >= n_surrogates:\n",
    "            print(\"E = {}\".format(optE))\n",
    "            print(\"Threshold = {}\".format(thres))\n",
    "            break\n",
    "    rng = default_rng()\n",
    "    return (\n",
    "        x_e,\n",
    "        rng.choice(a=surrogate_slice, size=n_surrogates, replace=False, axis=1),\n",
    "        optE,\n",
    "    )\n",
    "\n",
    "\n",
    "def generate_all_surrogates(df, mae_dict, n_surrogates=5, obs_per_year=12):\n",
    "    \"\"\"\n",
    "    Generates surrogates for all species of a given dataframe, using the :meth:`src.PLTS.networkccm3.run_surrogates`\n",
    "    method.\n",
    "    :param df: input dataframe\n",
    "    :param mae_dict: dictionary of lists; generate with :meth:`src.processing.embedding_dimension.mae_dictionary`\n",
    "    :param n_surrogates: number of surrogates to use with phase-lock twin surrogate method\n",
    "    :param obs_per_year: periodicity of time series data (12 for Guadalquivir data, 24 for Maizuru data)\n",
    "    :return: length of embedded dataframe, array of surrogate time series, array of optimal embedding dimensions\n",
    "    \"\"\"\n",
    "    species_list = list(df.columns[1:])\n",
    "    len_df = len(df)\n",
    "    surr_array = np.zeros((len(species_list), len_df, n_surrogates))\n",
    "    Es = np.zeros(len(species_list))\n",
    "    for i, sp in enumerate(species_list):\n",
    "        x_e, surr_array[i], Es[i] = run_surrogates(\n",
    "            df, mae_dict, sp, n_surrogates=n_surrogates, obs_per_year=obs_per_year\n",
    "        )\n",
    "    return x_e, surr_array, Es.astype(int)\n",
    "\n",
    "\n",
    "\n",
    "def mae_dictionary(df, max_dim=24):\n",
    "    \"\"\"\n",
    "    Creates dictionary with species as keys and each corresponding index a list of embedding dimensions from 1 to\n",
    "    max_dim in order of optimality according to mean absolute error. See section 3.2 of report.\n",
    "    :param df: input dataframe of usual form (e.g. that given by :meth:`src.abundance_tools.initialise_dataset`)\n",
    "    :param max_dim: maximum embedding dimension to include in dictionary. Default is 24, copying Ushio paper\n",
    "    :return: dictionary of optimal embedding dimension information\n",
    "    \"\"\"\n",
    "    mae_dict = {}\n",
    "    for _, sp in enumerate(df.columns[1:]):\n",
    "        MAEs = [[] for _ in range(2, max_dim + 1)]\n",
    "        for E in range(2, max_dim + 1):\n",
    "            library_string = \"1 {}\".format(len(df) - E)\n",
    "            preds = pyEDM.Simplex(\n",
    "                dataFrame=df,\n",
    "                columns=sp,\n",
    "                target=sp,\n",
    "                E=E,\n",
    "                Tp=1,\n",
    "                lib=library_string,\n",
    "                pred=library_string,\n",
    "                exclusionRadius=1,\n",
    "            )\n",
    "            MAEs[E - 2] = [\n",
    "                E,\n",
    "                np.nanmean(\n",
    "                    np.abs((preds[\"Predictions\"] - preds[\"Observations\"]).values)\n",
    "                ),\n",
    "            ]\n",
    "        mae_dict[sp] = MAEs\n",
    "    for sp in mae_dict:\n",
    "        mae_dict[sp].sort(key=takeSecond)\n",
    "    return mae_dict\n",
    "\n",
    "def takeSecond(elem):\n",
    "    \"\"\"\n",
    "    Returns second element of a list. Auxiliary function for :meth:`src.processing.embedding_dimension.mae_dictionary`\n",
    "    :param elem: list\n",
    "    :return: second element\n",
    "    \"\"\"\n",
    "    return elem[1]\n",
    "\n",
    "\n",
    "def twin_surrogate_esn(X, Y, Nsurr, period):\n",
    "    df = pd.DataFrame()\n",
    "    df['X'] = X\n",
    "    df['Y'] = Y\n",
    "    df.insert(loc=0, column=\"Time\", value=range(len(df)))\n",
    "    \n",
    "    df_mae_dict = mae_dictionary(df, max_dim=24)\n",
    "    _, surr_array, _ = generate_all_surrogates(df, df_mae_dict, n_surrogates = Nsurr, obs_per_year = period)\n",
    "    \n",
    "    return surr_array[0,:,:].T, surr_array[1,:,:].T\n",
    "\n",
    "def DeepESN_real(X, Y, n_data = 50, period = 1, verbose = False):\n",
    "    \n",
    "    # Get length of time series\n",
    "    N = X.shape[0]\n",
    "    \n",
    "    # compute surrogates\n",
    "    Xsurr, Ysurr = twin_surrogate_esn(X, Y, Nsurr = n_data, period = period)\n",
    "    \n",
    "    \n",
    "    # run the deepESN of the surrogates\n",
    "    xmapy_surr, ymapx_surr = compute_confidence(Xsurr, Ysurr, verbose)\n",
    "\n",
    "    # run deepESN for actual data\n",
    "    xmapy, ymapx = compute_lags(X, Y, n_data, verbose)\n",
    "\n",
    "    return xmapy_surr, ymapx_surr, xmapy, ymapx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5984d750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E = 2\n",
      "Threshold = 12.5\n",
      "E = 2\n",
      "Threshold = 12.5\n"
     ]
    }
   ],
   "source": [
    "xmapy_surr250, ymapx_surr250, xmapy250, ymapx250 = DeepESN_real(x_250, y_250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013113b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "xmapy_surr500, ymapx_surr500, xmapy500, ymapx500 = DeepESN_real(x_500, y_500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a7d3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "xmapy_surr1000, ymapx_surr1000, xmapy1000, ymapx1000 = DeepESN_real(x_1000, y_1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0207ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_fig(xmapy_surr250, ymapx_surr250, xmapy250, ymapx250, x_250, y_250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bef0f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_fig(xmapy_surr500, ymapx_surr500, xmapy500, ymapx500, x_500, y_500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01872498",
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_fig(xmapy_surr1000, ymapx_surr1000, xmapy1000, ymapx1000, x_1000, y_1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911c566c",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations250 = np.dstack((xmapy_surr250,ymapx_surr250,xmapy250,ymapx250))\n",
    "correlations500 = np.dstack((xmapy_surr500,ymapx_surr500,xmapy500,ymapx500))\n",
    "correlations1000 = np.dstack((xmapy_surr1000,ymapx_surr1000,xmapy1000,ymapx1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fc44e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('ModUni', correlations250)\n",
    "np.save('ModUni', correlations250)\n",
    "np.save('ModUni', correlations250)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m4r_venv",
   "language": "python",
   "name": "m4r_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
